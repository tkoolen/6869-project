#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\ad}{ad}
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\DeclareMathAlphabet{\mathsfb}{\encodingdefault}{\sfdefault}{bx}{n}
\DeclareMathAlphabet{\mathbbb}{U}{bbold}{m}{n}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman lmodern
\font_sans cmss
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Related work
\begin_inset CommandInset label
LatexCommand label
name "sec:Related-work"

\end_inset


\end_layout

\begin_layout Subsection
Block Matching
\end_layout

\begin_layout Standard
Block matching algorithms are used to find correspondences between blocks
 of pixels across frames.
 These algorithms are typically used in video compression systems to remove
 temporal redundancies when encoding sequences of frames.
 For a block of pixels 
\begin_inset Formula $B$
\end_inset

 in frame 
\series bold

\begin_inset Formula $I_{1}$
\end_inset


\series default
, block matching algorithms attempt to find the corresponding block in 
\begin_inset Formula $I_{2}$
\end_inset

 by solving the following optimization problem
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
(\hat{u},\hat{v})=\arg\min_{u,v}\underset{(x,y)\in B}{\sum}\rho(I_{2}(x+u,y+v),I_{1}(x,y))\label{eq:bma_formulation}
\end{equation}

\end_inset

where 
\begin_inset Formula $(\hat{u},\hat{v})$
\end_inset

 is the estimated displacement vector and 
\begin_inset Formula $\rho(\cdot,\cdot)$
\end_inset

 is a penalty function.
 Typical penalty functions include mean squared error, sum squared error,
 sum absolute error, normalized correlation, etc.
 Furthermore, there are several heuristics to reduce the computation time
 required to solve 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:bma_formulation"

\end_inset

.
 For an overview of these approaches, we refer the interested reader to
 
\begin_inset CommandInset citation
LatexCommand cite
key "jain1981displacement,lin1997fast"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Differential Methods
\end_layout

\begin_layout Standard
Differential optimization methods for optical flow estimation have a long
 history.
 In their seminal work 
\begin_inset CommandInset citation
LatexCommand cite
key "horn1981determining"

\end_inset

, Horn and Schunck developed one of the first such algorithms.
 The Horn-Schunk algorithm (HS) was based on two key assumptions.
 The first assumption is intensity constancy: in a sequence of images, changes
 in the intensity at a pixel location over time are only due to the movement
 of the surfaces in the scene and not due to changes in illumination.
 Mathematically, this can be expressed (in the discrete formulation) as
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
I_{1}\left(i,j\right)=I_{2}\left(i+u_{i,j},j+v_{i,j}\right)\label{eq:data-term}
\end{equation}

\end_inset

where 
\begin_inset Formula $I_{1}\left(i,j\right)$
\end_inset

 and 
\begin_inset Formula $I_{2}\left(i,j\right)$
\end_inset

 are the intensities at pixel 
\begin_inset Formula $\left(i,j\right)$
\end_inset

 in two consecutive frames and 
\begin_inset Formula $u_{i,j}$
\end_inset

 and 
\begin_inset Formula $v_{i,j}$
\end_inset

 are the horizontal and vertical components of the optical flow field at
 the pixel location.
\end_layout

\begin_layout Standard
Horn and Schunk recognized that the problem of finding the optical flow
 using only 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:data-term"

\end_inset

 is ill-posed.
 In regions where the intensity is constant (e.g.
 flat regions and along edges), there are several candidates for 
\begin_inset Formula $u_{i,j}$
\end_inset

 and 
\begin_inset Formula $v_{i,j}$
\end_inset

 that satisfy 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:data-term"

\end_inset

.
 Indeed, Bertero et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "bertero1988ill"

\end_inset

 show that only the optical flow component normal to edges can be determined
 directly from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:data-term"

\end_inset

.
 This is known as the aperture problem.
 
\end_layout

\begin_layout Standard
To address this, Horn and Schunk made a second assumption: the optical flow
 varies smoothly across the frame.
 This leads to an minimization problem with an objective function of the
 form
\begin_inset Formula 
\begin{align}
E\left(u,v\right) & =\sum_{i,j}\{\rho\left(I_{1}\left(i,j\right)-I_{2}\left(i+u_{i,j},j+v\left(i,j\right)\right)\right)\nonumber \\
 & +\lambda[\rho\left(u_{i,j}-u_{i+1,j}\right)+\rho\left(u_{i,j}-u_{i,j+1}\right)\nonumber \\
 & +\rho\left(v_{i,j}-v_{i+1,j}\right)+\rho\left(v_{i,j}-v_{i,j+1}\right)]\}\label{eq:objective}
\end{align}

\end_inset

where 
\begin_inset Formula $\rho\left(\cdot\right)$
\end_inset

 is a quadratic penalty function and 
\begin_inset Formula $\lambda$
\end_inset

 is a regularization parameter.
 The first term comes from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:data-term"

\end_inset

 and is called the data term, while the second regularizing term is called
 the smoothness term.
 Note that even though the penalty function is convex, minimizing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:objective"

\end_inset

 is a nonlinear problem since the image intensity 
\begin_inset Formula $I_{2}$
\end_inset

 is a nonlinear function of the flow field 
\begin_inset Formula $\left(u,v\right)$
\end_inset

, so local minima are a problem.
\end_layout

\begin_layout Standard
Modern approaches have improved the accuracy of differential optical flow
 algorithms.
 These updated approaches often changed the penalty function 
\begin_inset Formula $\rho\left(\cdot\right)$
\end_inset

, the optimization scheme and other aspects in minimizing an objective like
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:objective"

\end_inset

 all at once.
 Consequently, this has made it difficult to discern which changes significantly
 impacted accuracy.
 
\end_layout

\begin_layout Standard
Sun et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "sun2014quantitative"

\end_inset

 quantitatively analyzed which commonly used techniques truly improved the
 accuracy by varying only one part of the algorithm at a time and evaluating
 performance on the standard Middlebury dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "baker2011database"

\end_inset

.
 They independently analyzed the effects of the objective function and the
 optimization method.
 Sun et al.
 note that the objective function is largely unchanged from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:objective"

\end_inset

, used in HS.
 However, optimization techniques have changed more significantly.
 Using modern techniques, even the performance of the original HS objective
 function is competitive with modern formulations.
 To avoid converging to a poor local minimum, these modern optimization
 approaches iteratively refine the optimization problem as follows: 
\end_layout

\begin_layout Enumerate
If a non-convex penalty function is used, Sun et al.
 suggest using a graduated non-convexity scheme (GNC, 
\begin_inset CommandInset citation
LatexCommand cite
key "blake1987visual,black1996robust"

\end_inset

), which slowly morphs the penalty function from a quadratic (convex) version
 to its final desired non-convex version over several iterations.
\end_layout

\begin_layout Enumerate
At each GNC iteration, it is common to use a coarse-to-fine spatial refinement
 using a Gaussian pyramid 
\begin_inset CommandInset citation
LatexCommand cite
key "bergen1992hierarchical"

\end_inset

.
\end_layout

\begin_layout Standard
The implementation details of the coarse-to-fine refinement are of great
 importance.
 In particular, accuracy is increased significantly by applying a median
 filter to the updated flow field computed at each pyramid level after warping
 to remove outliers.
 Sun et al.
 combine the features that they find have the greatest effect on accuracy
 into a new algorithm they call 
\emph on
classic++
\emph default
.
 This algorithm, although comparatively simple, performs competitively on
 the Middlebury dataset, and will be used as a representative coarse-to-fine
 optimization method in our comparison.
\end_layout

\begin_layout Standard
Sun et al.
 go on to propose an improvement to the median filtering heuristic using
 structure from the image, leading to a weighted median filter, which will
 be described in more detail in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Weighted-median-filtering"

\end_inset

.
\end_layout

\end_body
\end_document
